# -*- coding: utf-8 -*-
"""SRGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W25WG_35nMYQ-fThSzAtLPANNl16-Zrb
"""

pip install torch torchvision

import os
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

class SRDataset(Dataset):
    def __init__(self, lr_dir, hr_dir, transform_lr=None, transform_hr=None):
        self.lr_dir = lr_dir
        self.hr_dir = hr_dir
        self.lr_images = sorted(os.listdir(lr_dir))
        self.hr_images = sorted(os.listdir(hr_dir))
        self.transform_lr = transform_lr
        self.transform_hr = transform_hr

    def __len__(self):
        return len(self.lr_images)

    def __getitem__(self, idx):
        lr_path = os.path.join(self.lr_dir, self.lr_images[idx])
        hr_path = os.path.join(self.hr_dir, self.hr_images[idx])

        lr_img = Image.open(lr_path).convert("RGB")
        hr_img = Image.open(hr_path).convert("RGB")

        if self.transform_lr:
            lr_img = self.transform_lr(lr_img)
        if self.transform_hr:
            hr_img = self.transform_hr(hr_img)

        return lr_img, hr_img

transform_lr = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

transform_hr = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

lr_dir = "./LR/LR"
hr_dir = "./HR/HR"

dataset = SRDataset(lr_dir, hr_dir, transform_lr, transform_hr)
loader = DataLoader(dataset, batch_size=4, shuffle=True)

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.block = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(channels),
            nn.PReLU(),
            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(channels)
        )
    def forward(self, x):
        return x + self.block(x)

class Generator(nn.Module):
    def __init__(self, num_residuals=16):
        super(Generator, self).__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4),
            nn.PReLU()
        )
        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residuals)])
        self.block2 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64)
        )
        self.upsample = nn.Sequential(
            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),
            nn.PixelShuffle(upscale_factor=2),
            nn.PReLU(),
            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),
            nn.PixelShuffle(upscale_factor=2),
            nn.PReLU()
        )
        self.block3 = nn.Conv2d(64, 3, kernel_size=9, stride=1, padding=4)

    def forward(self, x):
        x1 = self.block1(x)
        x2 = self.res_blocks(x1)
        x3 = self.block2(x2)
        x = x1 + x3
        x = self.upsample(x)
        x = self.block3(x)
        return torch.tanh(x)


class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        def block(in_channels, out_channels, stride):
            layers = [
                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.LeakyReLU(0.2, inplace=True)
            ]
            return layers

        self.net = nn.Sequential(
            *block(3, 64, 1),
            *block(64, 64, 2),
            *block(64, 128, 1),
            *block(128, 128, 2),
            *block(128, 256, 1),
            *block(256, 256, 2),
            *block(256, 512, 1),
            *block(512, 512, 2),
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(512, 1024, kernel_size=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(1024, 1, kernel_size=1)
        )

    def forward(self, x):
        return self.net(x).view(x.size(0), -1)

import torchvision.models as models

class VGGFeatureExtractor(nn.Module):
    def __init__(self):
        super(VGGFeatureExtractor, self).__init__()
        vgg19 = models.vgg19(pretrained=True)
        # Use features up to layer 35 (ReLU before 5th MaxPool)
        self.feature_extractor = nn.Sequential(*list(vgg19.features.children())[:36])
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, img):
        return self.feature_extractor(img)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
generator = Generator().to(device)
discriminator = Discriminator().to(device)
criterion = nn.MSELoss()
optimizer_G = optim.Adam(generator.parameters(), lr=1e-4)
optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4)

l1 = nn.L1Loss()
vgg = VGGFeatureExtractor().to(device)
bce = nn.BCEWithLogitsLoss()

pretrain_epochs = 50
for epoch in range(pretrain_epochs):
    loop = tqdm(loader, desc=f"Pretrain Epoch [{epoch+1}/{pretrain_epochs}]")
    for lr, hr in loop:
        lr, hr = lr.to(device), hr.to(device)
        sr = generator(lr)  # note: 'generator' not 'G' if you used that name
        pixel_loss = l1(sr, hr)

        optimizer_G.zero_grad()
        pixel_loss.backward()
        optimizer_G.step()

        loop.set_postfix(Pixel_Loss=pixel_loss.item())

epochs = 250
for epoch in range(epochs):
    loop = tqdm(loader, desc=f"Epoch [{epoch+1}/{epochs}]")
    for lr, hr in loop:
        lr, hr = lr.to(device), hr.to(device)

        ### Train Discriminator ###
        sr = generator(lr)
        real_out = discriminator(hr)
        fake_out = discriminator(sr.detach())

        # Standard GAN BCE losses (no relativistic average)
        d_loss_real = bce(real_out, torch.ones_like(real_out))
        d_loss_fake = bce(fake_out, torch.zeros_like(fake_out))
        d_loss = (d_loss_real + d_loss_fake) / 2

        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        ### Train Generator ###
        fake_out = discriminator(sr)

        # Adversarial loss
        g_adv = bce(fake_out, torch.ones_like(fake_out))
        # Perceptual loss (VGG)
        sr_vgg = vgg(sr)
        hr_vgg = vgg(hr)
        g_perceptual = l1(sr_vgg, hr_vgg)
        # Pixel loss
        g_pixel = l1(sr, hr)

        # Total generator loss
        g_loss = 1e-3 * g_adv + 0.006 * g_perceptual + g_pixel

        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

        loop.set_postfix(D_loss=d_loss.item(), G_loss=g_loss.item())

# === Save the Trained Models ===
torch.save(generator.state_dict(), "SRGAN_generator_tuned.pth")
torch.save(discriminator.state_dict(), "SRGAN_discriminator_tuned.pth")
print("Models saved")

from torchvision.utils import save_image

generator.eval()
output_dir = "./SR_output_tuned"
os.makedirs(output_dir, exist_ok=True)

with torch.no_grad():
    for idx, (lr_img, hr_img) in enumerate(dataset):
        lr_img = lr_img.unsqueeze(0).to(device)
        sr_img = generator(lr_img)
        save_image(sr_img, os.path.join(output_dir, f"sr_output_tuned_{idx}.png"))

from skimage.metrics import peak_signal_noise_ratio, structural_similarity
import numpy as np

# Example: Compare one image
sr_image = sr_img.squeeze().permute(1,2,0).cpu().numpy()
hr_image = hr_img.permute(1,2,0).cpu().numpy()

psnr = peak_signal_noise_ratio(hr_image, sr_image, data_range=1.0)
ssim = structural_similarity(hr_image, sr_image, data_range=1.0, channel_axis=-1)

print(f"PSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}")

